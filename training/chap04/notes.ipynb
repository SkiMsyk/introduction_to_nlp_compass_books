{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing  \n",
    "1. cleaning text\n",
    "1. split to words\n",
    "1. regularizing words\n",
    "1. remove swap word\n",
    "1. id-nize words\n",
    "1. padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cleaning text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_html(html, strip = False):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    text = soup.get_text(strip= strip)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '\\n\\n\\n    this is example. \\n  \\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n    this is example. \\n  \\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_html(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is example.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_html(html, strip = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"let's write documents with MKDocs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "def clean_hashtag(text):\n",
    "    cleaned_text = re.sub(r'#[a-zA-Z]+', '', text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hashtag(text):\n",
    "    cleaned_text = re.sub(r'( #[a-zA-Z]+)+$', '', text)\n",
    "    cleaned_text = re.sub(r' #(?P<tag>[a-zA-Z]+) ', r'\\g<tag>', cleaned_text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'機械学習やるならpythonがいいよね。'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_hashtag('機械学習やるなら #python がいいよね。 #jupyter #pycon #scipy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "彼女\t名詞,代名詞,一般,*,*,*,彼女,カノジョ,カノジョ\n",
      "と\t助詞,格助詞,一般,*,*,*,と,ト,ト\n",
      "国立\t名詞,一般,*,*,*,*,国立,コクリツ,コクリツ\n",
      "新\t接頭詞,名詞接続,*,*,*,*,新,シン,シン\n",
      "美術館\t名詞,一般,*,*,*,*,美術館,ビジュツカン,ビジュツカン\n",
      "ヘ\t助詞,格助詞,一般,*,*,*,ヘ,ヘ,エ\n",
      "行っ\t動詞,自立,*,*,五段・ワ行促音便,連用タ接続,行う,オコナッ,オコナッ\n",
      "た\t助動詞,*,*,*,特殊・タ,基本形,た,タ,タ\n",
      "。\t記号,句点,*,*,*,*,。,。,。\n"
     ]
    }
   ],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "text = '彼女と国立新美術館ヘ行った。'\n",
    "t = Tokenizer()\n",
    "for token in t.tokenize(text):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['彼女', 'と', '国立', '新', '美術館', 'ヘ', '行っ', 'た', '。']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = Tokenizer(wakati = True)\n",
    "t.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "彼女\t名詞,代名詞,一般,*,*,*,彼女,カノジョ,カノジョ\n",
      "国立\t名詞,一般,*,*,*,*,国立,コクリツ,コクリツ\n",
      "美術館\t名詞,一般,*,*,*,*,美術館,ビジュツカン,ビジュツカン\n"
     ]
    }
   ],
   "source": [
    "from janome.analyzer import Analyzer\n",
    "from janome.tokenfilter import POSKeepFilter\n",
    "\n",
    "token_filters = [POSKeepFilter('名詞')]\n",
    "a = Analyzer(token_filters=token_filters)\n",
    "for token in a.analyze(text):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add word to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data/userdic.csv', 'a') as f:\n",
    "#     f.write('国立新美術館,1288,1288,100,名詞,固有名詞,一般,*,*,*,国立新美術館,コクリツシンビジュツカン,コクリツシンビジュツカン')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "t = Tokenizer(udic = 'data/userdic.csv', udic_enc = 'utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "彼女\t名詞,代名詞,一般,*,*,*,彼女,カノジョ,カノジョ\n",
      "と\t助詞,並立助詞,*,*,*,*,と,ト,ト\n",
      "国立新美術館\t名詞,固有名詞,一般,*,*,*,国立新美術館,コクリツシンビジュツカン,コクリツシンビジュツカン\n",
      "ヘ\t助詞,格助詞,一般,*,*,*,ヘ,ヘ,エ\n",
      "行っ\t動詞,自立,*,*,五段・ワ行促音便,連用タ接続,行う,オコナッ,オコナッ\n",
      "た\t助動詞,*,*,*,特殊・タ,基本形,た,タ,タ\n",
      "。\t記号,句点,*,*,*,*,。,。,。\n"
     ]
    }
   ],
   "source": [
    "for token in t.tokenize(text):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unification of character types\n",
    "upper-case to lower-case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'president obama is speaking at the white house.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'President Obama is speaking at the White House.'\n",
    "text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PRESIDENT OBAMA IS SPEAKING AT THE WHITE HOUSE.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'President Obama Is Speaking At The White House.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.title()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace disit-numbers  \n",
    "if we assume that digit-numbers are not useful for our task (example for news classification in 'sports', 'politics'), we should replace them with other symbol.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "def normalize_number(text):\n",
    "    return re.sub(r'\\d+', '0', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0万0.0ドル'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '2万0689.24ドル'\n",
    "normalize_number(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you want not to change number of digit-numbers, you can processed how you want as bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_number(text):\n",
    "    return re.sub(r'\\d', '0', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0万0000.00ドル'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_number(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unification of words with common-means using dictionary  \n",
    "There are in sentences frequently that words which have different spell but same mean.  \n",
    "In such situation, we can use modified dictionary to change them to same spell.  \n",
    "\n",
    "ex)\n",
    "* ソニー, Sony, sony \n",
    "* パナソニック, Panasonic, Pana, 松下電器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove stop-words  \n",
    "Stop-words mean not useful-words for NLP tasks. For example, 「は」, 「の」．  \n",
    "they are exist frequently and have bad effects fot preprocess calculation and quality.   \n",
    "\n",
    "methods:\n",
    "1. using dictionary\n",
    "1. using frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove using dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# url = 'http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt'\n",
    "# content_text = requests.get(url).content.decode('utf-8')\n",
    "# with open('data/Japanese.txt', 'a') as f:\n",
    "#     f.write(content_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/Japanese.txt', 'r', encoding = 'utf-8') as f:\n",
    "    stopwords = [w.strip() for w in f]\n",
    "    stopwords = set(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(words, stopwords):\n",
    "    return [w for w in words if w not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['りんご', 'を', 'いくつ', 'か', '買う']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "t = Tokenizer(wakati = True)\n",
    "text = 'りんごをいくつか買う'\n",
    "words = t.tokenize(text)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['りんご', 'を', 'か', '買う']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(words, stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove using frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we should stop word list from text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'https://s3-ap-northeast-1.amazonaws.com/dev.tech-sketch.jp/chakki/public/ja.text8.zip'\n",
    "dl_file = requests.get(url, stream=True)\n",
    "with open('data/ja_text.zip', 'wb') as f:\n",
    "    for chunk in dl_file.iter_content(chunk_size = 1024):\n",
    "        if chunk:\n",
    "            f.write(chunk)\n",
    "            f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already exists\n"
     ]
    }
   ],
   "source": [
    "# zipファイルを展開\n",
    "import zipfile\n",
    "import os\n",
    "target_directory = 'data'\n",
    "zfile = zipfile.ZipFile('data/ja_text.zip')\n",
    "if 'ja.text8' not in os.listdir('data'):\n",
    "    zfile.extractall(target_directory)\n",
    "else:\n",
    "    print('already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/ja.text8', 'r', encoding = 'utf-8') as f:\n",
    "    text = f.read()\n",
    "    words = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'cat': 2, 'dog': 1})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter \n",
    "Counter(['cat', 'dog', 'cat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('の', 828585),\n",
       " ('、', 785716),\n",
       " ('。', 532921),\n",
       " ('に', 527014),\n",
       " ('は', 488009),\n",
       " ('を', 423115),\n",
       " ('た', 421908),\n",
       " ('が', 353221),\n",
       " ('で', 350821),\n",
       " ('て', 259995)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist.most_common(n = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ID conversion  \n",
    "allocate id-number to a word and replace word by its id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK = '<UNK>' # for unknown word\n",
    "PAD = '<PAD>' # for padding process\n",
    "vocab = {PAD:0, UNK:1}\n",
    "for word, _ in fdist.most_common():\n",
    "    vocab[word] = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1151, 6, 7901]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['私', 'は', '元気']\n",
    "word_ids = [vocab.get(w, vocab[UNK]) for w in words]\n",
    "word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 2],\n",
       "       [0, 3, 4, 5],\n",
       "       [6, 7, 8, 9]], dtype=int32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "sequences = [[1,2], [3,4,5], [6,7,8,9]]\n",
    "pad_sequences(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 0, 0],\n",
       "       [3, 4, 5, 0],\n",
       "       [6, 7, 8, 9]], dtype=int32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequences(sequences, padding = 'post') #あと詰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [3, 4, 5],\n",
       "       [7, 8, 9]], dtype=int32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequences(sequences, maxlen = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [3, 4, 5],\n",
       "       [6, 7, 8]], dtype=int32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequences(sequences, maxlen = 3, truncating = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[99, 99,  1,  2],\n",
       "       [99,  3,  4,  5],\n",
       "       [ 6,  7,  8,  9]], dtype=int32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequences(sequences, value = 99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Split word\n",
    "* Cleaning HTML-tag\n",
    "* Regularization of digit-numbers\n",
    "* Word conversion to original form\n",
    "* Character conversion to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
